{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza. Liza, what kind of doggy is she? She's a lad. Oh, my God, I love those. Those are so beautiful. Is she a black lad or is she a yellow lad? A black lad. Oh, Mike, you are so smart. This brown eyes. Oh, my goodness, hold it. She is like three years old or two years old. I don't really know. Wow! Have you had, you can remember for a long time, huh? Yes. Oh, my goodness. Did you, was there anybody else in the house besides you, a mommy and daddy tonight? Like an uncle or anything? No, it's like you're a robber and now. Okay, well, I didn't think there's the robbers, sweetheart. Did you have anybody staying over with you guys tonight? Mm-hmm. Okay. Staying over with you guys tonight? Mm-hmm. Okay. So, and the doors are locked and everything like that, where are you in the house? Well, I was in my room sleeping until I had heard a noise shot in the room. Oh, my goodness. What's the house are you in at now? I'm in the one at the yellow house of green. Green with a little bit of green and a pink door. Oh, my goodness. That sounds really cool. Did you pick out the pink door? No, my mom picked out the car. Look, that's beautiful. Does mommy and daddy have a car in the driveway? Two cars. Two cars? What kind of car is the big ham? My mom has a Toyota. What car is that Toyota? Is it dark or gray or silver? It's red. Red? Okay. What's the car the daddy has? He has a diva. It's black and red. Listen to me. Is your phone the kind of phone you can take with you and walk around? Yes. There should be an officer at your front door. I need for you to take your phone with you and walk over to the door and open it for me. And I'm going to stand the phone with you. I will not hang up. I'll make it. Oh, well, do you want to grab a towel or something? I don't think the officer is going to care, baby. We just want to make sure mommy daddy is okay, all right? Grab a blanket or something. Stand the phone with me. Stand the phone, all right? Okay. Money's done up, by the way. You are doing a wonderful job. Wonderful job. And I know what to do for this, is that? You did. Right. I knew. You are wonderful. Absolutely wonderful. You should be very proud of yourself. And, is it the oil monlock? Okay. You do. Let me know when the officer talks to you. Okay. You talk to the officer. Talk to the dispatcher. Okay. Tell her I'm here now. Okay. You can hang up. Bye. Sweetheart. Okay. Sweetheart. You'd be good, okay? Come on over here. Bye-bye.\", metadata={'source': '/home/david/Documents/hackathon2024/data/transcription_call_8.txt'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Path to your .txt file\n",
    "file_path = \"/home/david/Documents/hackathon2024/data/transcription_call_8.txt\"\n",
    "\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "loader = TextLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza. Liza, what kind of doggy is she? She's a lad. Oh, my God, I love those. Those are so beautiful. Is she a black lad or is she a yellow lad? A black lad. Oh, Mike, you are so smart. This brown eyes. Oh, my goodness, hold it. She is like three years old or two years old. I don't really know. Wow! Have you had, you can remember for a long time, huh? Yes. Oh, my goodness. Did you, was there anybody else in the house besides you, a mommy and daddy tonight? Like an uncle or anything? No, it's like you're a robber and now. Okay, well, I didn't think there's the robbers, sweetheart. Did you have anybody staying over with you guys tonight? Mm-hmm. Okay. Staying over with you guys tonight? Mm-hmm. Okay. So, and the doors are locked and everything like that, where are you in the house? Well, I was in my room sleeping until I had heard a noise shot in the room. Oh, my goodness. What's the house are you in at now? I'm in the one at the yellow house of green. Green with a little bit of green and a pink door. Oh, my goodness. That sounds really cool. Did you pick out the pink door? No, my mom picked out the car. Look, that's beautiful. Does mommy and daddy have a car in the driveway? Two cars. Two cars? What kind of car is the big ham? My mom has a Toyota. What car is that Toyota? Is it dark or gray or silver? It's red. Red? Okay. What's the car the daddy has? He has a diva. It's black and red. Listen to me. Is your phone the kind of phone you can take with you and walk around? Yes. There should be an officer at your front door. I need for you to take your phone with you and walk over to the door and open it for me. And I'm going to stand the phone with you. I will not hang up. I'll make it. Oh, well, do you want to grab a towel or something? I don't think the officer is going to care, baby. We just want to make sure mommy daddy is okay, all right? Grab a blanket or something. Stand the phone with me. Stand the phone, all right? Okay. Money's done up, by the way. You are doing a wonderful job. Wonderful job. And I know what to do for this, is that? You did. Right. I knew. You are wonderful. Absolutely wonderful. You should be very proud of yourself. And, is it the oil monlock? Okay. You do. Let me know when the officer talks to you. Okay. You talk to the officer. Talk to the dispatcher. Okay. Tell her I'm here now. Okay. You can hang up. Bye. Sweetheart. Okay. Sweetheart. You'd be good, okay? Come on over here. Bye-bye.\n"
     ]
    }
   ],
   "source": [
    "texts = docs[0].page_content\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5-year-old child calls 911 after finding their parents dead in their home. The child is scared and confused, but tries to provide information to the dispatcher about what happened. They mention seeing a bullet on the floor and a slide coming out of their father's mouth. The child also talks about their dog, Liza, and mentions that they were sleeping when they heard a noise. The dispatcher tries to reassure the child and asks them to stand up for her at the front door, where an officer will be waiting.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "local_llm = \"gpt-4-0125-preview\"\n",
    "llm = ChatOpenAI(\n",
    "    model = local_llm,\n",
    "    base_url = \"http://10.128.138.175:11434/v1/\",\n",
    "    temperature=0,\n",
    "    api_key=\"ollama\",\n",
    "    streaming=True,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "result = chain.invoke(docs)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/cira-project/code-cira/tools-langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5-year-old child calls 911 after finding their parents dead in their home. The child is shaken and confused, but tries to provide information about the situation. They report seeing a bullet on the floor and a slide coming out of their father's mouth. The operator tries to comfort the child and gather more information, including the location of the house and the presence of pets (a dog named Liza and three cats). The child is eventually instructed to open the front door for an officer who will arrive shortly.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()\n",
    "output = stuff_chain.invoke(docs)[\"output_text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your model weights\n",
    "local_model = \"/home/david/Documents/cira-project/llmodel/ggufModel/model/llama-2-7b-chat-dpo.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/david/Documents/cira-project/llmodel/ggufModel/model/llama-2-7b-chat-dpo.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 300\n",
      "llama_new_context_with_model: n_ubatch   = 300\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  5008.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 5008.00 MiB, K (f16): 2504.00 MiB, V (f16): 2504.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   397.78 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "llm_cpp = ChatLlamaCpp(\n",
    "    temperature=0.5,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=8,\n",
    "    n_batch=300,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'IncidentClassification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
    "    )\n",
    "    language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "class IncidentClassification(BaseModel):\n",
    "    violence: str = Field(description=\"Whether the incident involved violence\")\n",
    "    fire: str = Field(description=\"Whether the incident involved fire\")\n",
    "    medical: str = Field(description=\"Whether the incident involved medical attention\")      \n",
    "    suspicious_activity: str = Field(description=\"Whether the incident involved suspicious activity\")                                                                                         \n",
    "\n",
    "\n",
    "# LLM\n",
    "local_llm = \"gpt-4-0125-preview\"\n",
    "llm_original = ChatOpenAI(\n",
    "    model = local_llm,\n",
    "    base_url = \"http://10.128.138.175:11434/v1/\",\n",
    "    temperature=0,\n",
    "    api_key=\"ollama\",\n",
    "    streaming=True,\n",
    ")\n",
    "# llm = llm_original.with_structured_output(\n",
    "#     IncidentClassification\n",
    "# )\n",
    "\n",
    "llm = llm_cpp.with_structured_output(\n",
    "    IncidentClassification\n",
    ")\n",
    "\n",
    "tagging_chain = tagging_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A 5-year-old child calls 911 after finding their parents dead in their home. The child is shaken and confused, but tries to provide information about the situation. They report seeing a bullet on the floor and a slide coming out of their father's mouth. The operator tries to comfort the child and gather more information, including the location of the house and the presence of pets (a dog named Liza and three cats). The child is eventually instructed to open the front door for an officer who will arrive shortly.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "fire-kv ::= [\"] [f] [i] [r] [e] [\"] space [:] space string \n",
      "space ::= space_9 \n",
      "string ::= [\"] string_10 [\"] space \n",
      "medical-kv ::= [\"] [m] [e] [d] [i] [c] [a] [l] [\"] space [:] space string \n",
      "root ::= [{] space violence-kv [,] space fire-kv [,] space medical-kv [,] space suspicious-activity-kv [}] space \n",
      "violence-kv ::= [\"] [v] [i] [o] [l] [e] [n] [c] [e] [\"] space [:] space string \n",
      "suspicious-activity-kv ::= [\"] [s] [u] [s] [p] [i] [c] [i] [o] [u] [s] [_] [a] [c] [t] [i] [v] [i] [t] [y] [\"] space [:] space string \n",
      "space_9 ::= [ ] | \n",
      "string_10 ::= char string_10 | \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7958.00 ms\n",
      "llama_print_timings:      sample time =      86.66 ms /    33 runs   (    2.63 ms per token,   380.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7957.42 ms /   155 tokens (   51.34 ms per token,    19.48 tokens per second)\n",
      "llama_print_timings:        eval time =    8467.33 ms /    32 runs   (  264.60 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:       total time =   16547.35 ms /   187 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncidentClassification(violence='bullets', fire='', medical='', suspicious_activity='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "res = tagging_chain.invoke({\"input\": output})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'IncidentInformation' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "class IncidentInformation(BaseModel):\n",
    "    who: str = Field(description=\"Who was involved in the incident\")\n",
    "    what: str = Field(description=\"What happened during the incident\")\n",
    "    when: str = Field(description=\"When did the incident occur\")\n",
    "    where: str = Field(description=\"Where did the incident occur\")\n",
    "    why: str = Field(description=\"Why did the incident occur\")\n",
    "\n",
    "\n",
    "info_llm = llm_cpp.with_structured_output(\n",
    "    IncidentInformation\n",
    ")\n",
    "\n",
    "info_tagging_chain = info_tagging_prompt | info_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza. Liza, what kind of doggy is she? She's a lad. Oh, my God, I love those. Those are so beautiful. Is she a black lad or is she a yellow lad? A black lad. Oh, Mike, you are so smart. This brown eyes. Oh, my goodness, hold it. She is like three years old or two years old. I don't really know. Wow! Have you had, you can remember for a long time, huh? Yes. Oh, my goodness. Did you, was there anybody else in the house besides you, a mommy and daddy tonight? Like an uncle or anything? No, it's like you're a robber and now. Okay, well, I didn't think there's the robbers, sweetheart. Did you have anybody staying over with you guys tonight? Mm-hmm. Okay. Staying over with you guys tonight? Mm-hmm. Okay. So, and the doors are locked and everything like that, where are you in the house? Well, I was in my room sleeping until I had heard a noise shot in the room. Oh, my goodness. What's the house are you in at now? I'm in the one at the yellow house of green. Green with a little bit of green and a pink door. Oh, my goodness. That sounds really cool. Did you pick out the pink door? No, my mom picked out the car. Look, that's beautiful. Does mommy and daddy have a car in the driveway? Two cars. Two cars? What kind of car is the big ham? My mom has a Toyota. What car is that Toyota? Is it dark or gray or silver? It's red. Red? Okay. What's the car the daddy has? He has a diva. It's black and red. Listen to me. Is your phone the kind of phone you can take with you and walk around? Yes. There should be an officer at your front door. I need for you to take your phone with you and walk over to the door and open it for me. And I'm going to stand the phone with you. I will not hang up. I'll make it. Oh, well, do you want to grab a towel or something? I don't think the officer is going to care, baby. We just want to make sure mommy daddy is okay, all right? Grab a blanket or something. Stand the phone with me. Stand the phone, all right? Okay. Money's done up, by the way. You are doing a wonderful job. Wonderful job. And I know what to do for this, is that? You did. Right. I knew. You are wonderful. Absolutely wonderful. You should be very proud of yourself. And, is it the oil monlock? Okay. You do. Let me know when the officer talks to you. Okay. You talk to the officer. Talk to the dispatcher. Okay. Tell her I'm here now. Okay. You can hang up. Bye. Sweetheart. Okay. Sweetheart. You'd be good, okay? Come on over here. Bye-bye.\", metadata={'source': '/home/david/Documents/hackathon2024/data/transcription_call_8.txt'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "root ::= [{] space who-kv [,] space what-kv [,] space when-kv [,] space where-kv [,] space why-kv [}] space \n",
      "space ::= space_9 \n",
      "who-kv ::= [\"] [w] [h] [o] [\"] space [:] space string \n",
      "what-kv ::= [\"] [w] [h] [a] [t] [\"] space [:] space string \n",
      "when-kv ::= [\"] [w] [h] [e] [n] [\"] space [:] space string \n",
      "where-kv ::= [\"] [w] [h] [e] [r] [e] [\"] space [:] space string \n",
      "why-kv ::= [\"] [w] [h] [y] [\"] space [:] space string \n",
      "space_9 ::= [ ] | \n",
      "string ::= [\"] string_11 [\"] space \n",
      "string_11 ::= char string_11 | \n",
      "\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7958.00 ms\n",
      "llama_print_timings:      sample time =     130.59 ms /    46 runs   (    2.84 ms per token,   352.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   71000.77 ms /  1306 tokens (   54.37 ms per token,    18.39 tokens per second)\n",
      "llama_print_timings:        eval time =   12921.49 ms /    45 runs   (  287.14 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:       total time =   84107.13 ms /  1351 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncidentInformation(who='mommy and daddy', what='gun shot', when='tonight', where='the house', why='I don’t know')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "info_res = info_tagging_chain.invoke({\"input\": docs})\n",
    "info_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Context Reorder\n",
    "\n",
    "reduce the important information degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  sentence-transformers langchain-chroma langchain langchain-openai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/cira-project/code-cira/tools-langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/david/Documents/cira-project/code-cira/tools-langchain/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/david/Documents/cira-project/code-cira/tools-langchain/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.document_transformers import (\n",
    "#     LongContextReorder,\n",
    "# )\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_openai import OpenAI\n",
    "\n",
    "# # Get embeddings.\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # texts = [\n",
    "# #     \"Basquetball is a great sport.\",\n",
    "# #     \"Fly me to the moon is one of my favourite songs.\",\n",
    "# #     \"The Celtics are my favourite team.\",\n",
    "# #     \"This is a document about the Boston Celtics\",\n",
    "# #     \"I simply love going to the movies\",\n",
    "# #     \"The Boston Celtics won the game by 20 points\",\n",
    "# #     \"This is just a random text.\",\n",
    "# #     \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "# #     \"L. Kornet is one of the best Celtics players.\",\n",
    "# #     \"Larry Bird was an iconic NBA player.\",\n",
    "# # ]\n",
    "\n",
    "# # Create a retriever\n",
    "# retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "#     search_kwargs={\"k\": 10}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x74b3505217c0>, search_kwargs={'k': 10})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='S'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s'),\n",
       " Document(page_content='s')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# # Get relevant documents ordered by relevance score\n",
    "# docs = retriever.invoke(\"summarize\")\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
