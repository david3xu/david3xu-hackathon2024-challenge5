{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza.\", metadata={'source': '/home/david/Documents/hackathon2024/splitData/one.txt'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Path to your .txt file\n",
    "file_path = \"/home/david/Documents/hackathon2024/splitData/one.txt\"\n",
    "\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "loader = TextLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza.\n"
     ]
    }
   ],
   "source": [
    "texts = docs[0].page_content\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5-year-old child calls 911 after finding their parents' bodies on the floor with a bullet nearby. The child is shaken and confused, but tries to provide information about what they saw. The 911 operator attempts to comfort the child and gather more details, including the names of the child's pets (Liza the dog and three cats).\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "local_llm = \"gpt-4-0125-preview\"\n",
    "llm = ChatOpenAI(\n",
    "    model = local_llm,\n",
    "    base_url = \"http://10.128.138.175:11434/v1/\",\n",
    "    temperature=0,\n",
    "    api_key=\"ollama\",\n",
    "    streaming=True,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "result = chain.invoke(docs)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/gen-ui-python/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5-year-old child calls 911 after finding their parents' bodies on the floor with a bullet nearby. The child is shaken and confused, but able to provide some details about the scene. The dispatcher tries to comfort the child and gather more information, including the names of the child's pets (Liza the dog and three cats). The child is eventually convinced to stand up and wait for help to arrive.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()\n",
    "output = stuff_chain.invoke(docs)[\"output_text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your model weights\n",
    "local_model = \"/home/david/Documents/cira-project/llmodel/ggufModel/model/llama-2-7b-chat-dpo.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall langchain_community\n",
    "# %pip install langchain_community\n",
    "# %pip install --upgrade langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/david/Documents/cira-project/llmodel/ggufModel/model/llama-2-7b-chat-dpo.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 300\n",
      "llama_new_context_with_model: n_ubatch   = 300\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  5008.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 5008.00 MiB, K (f16): 2504.00 MiB, V (f16): 2504.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   397.78 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "llm_cpp = ChatLlamaCpp(\n",
    "    temperature=0.5,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=8,\n",
    "    n_batch=300,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'IncidentClassification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
    "    )\n",
    "    language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "class IncidentClassification(BaseModel):\n",
    "    violence: str = Field(description=\"Whether the incident involved violence\")\n",
    "    fire: str = Field(description=\"Whether the incident involved fire\")\n",
    "    medical: str = Field(description=\"Whether the incident involved medical attention\")      \n",
    "    suspicious_activity: str = Field(description=\"Whether the incident involved suspicious activity\")                                                                                         \n",
    "\n",
    "\n",
    "# LLM\n",
    "local_llm = \"gpt-4-0125-preview\"\n",
    "llm_original = ChatOpenAI(\n",
    "    model = local_llm,\n",
    "    base_url = \"http://10.128.138.175:11434/v1/\",\n",
    "    temperature=0,\n",
    "    api_key=\"ollama\",\n",
    "    streaming=True,\n",
    ")\n",
    "# llm = llm_original.with_structured_output(\n",
    "#     IncidentClassification\n",
    "# )\n",
    "\n",
    "llm = llm_cpp.with_structured_output(\n",
    "    IncidentClassification\n",
    ")\n",
    "\n",
    "tagging_chain = tagging_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A 5-year-old child calls 911 after finding their parents' bodies on the floor with a bullet nearby. The child is shaken and confused, but able to provide some details about the scene. The dispatcher tries to comfort the child and gather more information, including the names of the child's pets (Liza the dog and three cats). The child is eventually convinced to stand up and wait for help to arrive.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "fire-kv ::= [\"] [f] [i] [r] [e] [\"] space [:] space string \n",
      "space ::= space_9 \n",
      "string ::= [\"] string_10 [\"] space \n",
      "medical-kv ::= [\"] [m] [e] [d] [i] [c] [a] [l] [\"] space [:] space string \n",
      "root ::= [{] space violence-kv [,] space fire-kv [,] space medical-kv [,] space suspicious-activity-kv [}] space \n",
      "violence-kv ::= [\"] [v] [i] [o] [l] [e] [n] [c] [e] [\"] space [:] space string \n",
      "suspicious-activity-kv ::= [\"] [s] [u] [s] [p] [i] [c] [i] [o] [u] [s] [_] [a] [c] [t] [i] [v] [i] [t] [y] [\"] space [:] space string \n",
      "space_9 ::= [ ] | \n",
      "string_10 ::= char string_10 | \n",
      "\n",
      "\n",
      "llama_print_timings:        load time =    7056.23 ms\n",
      "llama_print_timings:      sample time =     108.79 ms /    38 runs   (    2.86 ms per token,   349.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7055.41 ms /   135 tokens (   52.26 ms per token,    19.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10432.99 ms /    37 runs   (  281.97 ms per token,     3.55 tokens per second)\n",
      "llama_print_timings:       total time =   17644.18 ms /   172 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncidentClassification(violence='yes', fire='no', medical='Yes(child injured)', suspicious_activity='No')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "res = tagging_chain.invoke({\"input\": output})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'IncidentInformation' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "class IncidentInformation(BaseModel):\n",
    "    who: str = Field(description=\"Who was involved in the incident\")\n",
    "    what: str = Field(description=\"What happened during the incident\")\n",
    "    when: str = Field(description=\"When did the incident occur\")\n",
    "    where: str = Field(description=\"Where did the incident occur\")\n",
    "    why: str = Field(description=\"Why did the incident occur\")\n",
    "\n",
    "\n",
    "info_llm = llm_cpp.with_structured_output(\n",
    "    IncidentInformation\n",
    ")\n",
    "\n",
    "info_tagging_chain = info_tagging_prompt | info_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza.\", metadata={'source': '/home/david/Documents/hackathon2024/splitData/one.txt'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "root ::= [{] space who-kv [,] space what-kv [,] space when-kv [,] space where-kv [,] space why-kv [}] space \n",
      "space ::= space_9 \n",
      "who-kv ::= [\"] [w] [h] [o] [\"] space [:] space string \n",
      "what-kv ::= [\"] [w] [h] [a] [t] [\"] space [:] space string \n",
      "when-kv ::= [\"] [w] [h] [e] [n] [\"] space [:] space string \n",
      "where-kv ::= [\"] [w] [h] [e] [r] [e] [\"] space [:] space string \n",
      "why-kv ::= [\"] [w] [h] [y] [\"] space [:] space string \n",
      "space_9 ::= [ ] | \n",
      "string ::= [\"] string_11 [\"] space \n",
      "string_11 ::= char string_11 | \n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7056.23 ms\n",
      "llama_print_timings:      sample time =     253.77 ms /    81 runs   (    3.13 ms per token,   319.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36508.80 ms /   592 tokens (   61.67 ms per token,    16.22 tokens per second)\n",
      "llama_print_timings:        eval time =   26362.26 ms /    80 runs   (  329.53 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =   63231.78 ms /   672 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncidentInformation(who='mommy and daddy', what='a bullet on the floor, a slide coming out of my dadâ€™s mouth... blood on plant &amp; floors.', when='tonight', where=\". I think they're dead.\", why=\".I don't know. Okay, dude....\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "info_res = info_tagging_chain.invoke({\"input\": docs})\n",
    "info_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index\n",
    "# %pip install spacy\n",
    "# %pip install llama-index-llms-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p 'data/paul_graham/'\n",
    "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='6ff0663f-d046-4e45-8b78-905066d6eaf2', embedding=None, metadata={'file_path': '/home/david/Documents/hackathon2024/splitData/one.txt', 'file_name': 'one.txt', 'file_type': 'text/plain', 'file_size': 1736, 'creation_date': '2024-06-23', 'last_modified_date': '2024-06-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reader = SimpleDirectoryReader(\"./data/paul_graham/\")\n",
    "reader = SimpleDirectoryReader(\"/home/david/Documents/hackathon2024/splitData/\")\n",
    "documents = reader.load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_doc = ChatOpenAI(\n",
    "    model = local_llm,\n",
    "    base_url = \"http://10.128.138.175:11434/v1/\",\n",
    "    temperature=0,\n",
    "    api_key=\"ollama\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" No one works for emergency. Um, hello. Hello. Is everything okay? Um, my mommy and daddy. Uh-huh. I think there's a bullet on the floor. Is there the what? And there's a... There's a slide coming out of my dad's mouth and he fell off the bed. He did? Where's mommy at? I... I don't know. I think they're dead. What do you mean, sweetheart? I don't know. Okay, dude. Your daddy's on the floor. How old are you? I'm five years old and I have a dog in the house. Okay, there, there, okay. Let me get something right over to you. Did you go in your mommy and daddy's room? Oh, and there's a blood. Oh, over the place? Not all over. There's blood on the plant and blood on the floor. Oh, my goodness. You have your little doggy with you? And three cats. Well, get some cats too. Three cats and one dog. Okay. Are you the only one there? Is this like mommy and daddy? Oh, my... It's dad and mommy and daddy and they didn't even answer. Okay. Okay, what I want you to do, honey, I want you to stand up for me. What is your name? Oh, that's a very pretty name. Okay. Oh, I'm so sorry. That is a beautiful name. What's your doggy's name? Liza. Okay. And what makes you wake up tonight? Okay. And what makes you wake up tonight? There was... I think I had a gun shot. You heard a gun? Yes, I see a bullet laying on it. Well, I think it's a bullet. Really? Do you? Who has a gun in the house? I don't see a gun, but scared. Oh, sweetheart, I will not let anything happen to you. Can you send me a message? I promise I will. You're only five years old. You are so smart for five years old. Wow! Are you off of school this week? No, I go to school next year. Oh, my gosh, you're not even in kindergarten yet? No. Oh, what's your doggy's name? Liza.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/gen-ui-python/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/dataset_generation.py:215: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "data_generator = DatasetGenerator.from_documents(\n",
    "    llm=llm_doc,\n",
    "    documents=documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.evaluation.dataset_generation.DatasetGenerator at 0x7b0504ce3b20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/gen-ui-python/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://10.128.138.175:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://10.128.138.175:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/gen-ui-python/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/dataset_generation.py:312: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Here are 10 questions based on the given context information:',\n",
       " 'What is the file path of the text file mentioned in the context?',\n",
       " 'Answer: /home/david/Documents/hackathon20/24/splitData/one.txt',\n",
       " 'What is the type of the file mentioned in the context?',\n",
       " 'Answer: text/plain']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions = data_generator.generate_questions_from_nodes(\n",
    "    num=5,\n",
    ")\n",
    "eval_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama_index.llms.fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator_gpt4 = RelevancyEvaluator(llm=llm_doc)\n",
    "# evaluator_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create vector index\n",
    "# vector_index = VectorStoreIndex.from_documents(\n",
    "#     # llm=llm_doc,\n",
    "#     documents=documents\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define jupyter display function\n",
    "# def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
    "#     eval_df = pd.DataFrame(\n",
    "#         {\n",
    "#             \"Query\": query,\n",
    "#             \"Response\": str(response),\n",
    "#             \"Source\": (\n",
    "#                 response.source_nodes[0].node.get_content()[:1000] + \"...\"\n",
    "#             ),\n",
    "#             \"Evaluation Result\": eval_result,\n",
    "#         },\n",
    "#         index=[0],\n",
    "#     )\n",
    "#     eval_df = eval_df.style.set_properties(\n",
    "#         **{\n",
    "#             \"inline-size\": \"600px\",\n",
    "#             \"overflow-wrap\": \"break-word\",\n",
    "#         },\n",
    "#         subset=[\"Response\", \"Source\"]\n",
    "#     )\n",
    "#     display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = vector_index.as_query_engine()\n",
    "# response_vector = query_engine.query(eval_questions[1])\n",
    "# eval_result = evaluator_gpt4.evaluate_response(\n",
    "#     query=eval_questions[1], response=response_vector\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Context Reorder\n",
    "\n",
    "reduce the important information degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  sentence-transformers langchain-chroma langchain langchain-openai > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.document_transformers import (\n",
    "#     LongContextReorder,\n",
    "# )\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_openai import OpenAI\n",
    "\n",
    "# # Get embeddings.\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # texts = [\n",
    "# #     \"Basquetball is a great sport.\",\n",
    "# #     \"Fly me to the moon is one of my favourite songs.\",\n",
    "# #     \"The Celtics are my favourite team.\",\n",
    "# #     \"This is a document about the Boston Celtics\",\n",
    "# #     \"I simply love going to the movies\",\n",
    "# #     \"The Boston Celtics won the game by 20 points\",\n",
    "# #     \"This is just a random text.\",\n",
    "# #     \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "# #     \"L. Kornet is one of the best Celtics players.\",\n",
    "# #     \"Larry Bird was an iconic NBA player.\",\n",
    "# # ]\n",
    "\n",
    "# # Create a retriever\n",
    "# retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "#     search_kwargs={\"k\": 10}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# # Get relevant documents ordered by relevance score\n",
    "# docs = retriever.invoke(\"summarize\")\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
